#!/usr/bin/env python
import client
import argparse
import cmd
import signal
import sys
import json
import os
import textwrap
import json
import dateutil.parser
import time
from util import hasattrs, read_api_key_and_secret_or_die, api_key_name, api_secret_name, path_name
from subprocess import call

parser = argparse.ArgumentParser(prog = "stackmob-parse-importer", description='move data from Parse to StackMob')
parser.add_argument("--%s"%("api_key"), help="the API key for your app", metavar="api-key", required=False, dest=api_key_name)
parser.add_argument("--%s"%("api_secret"), help="the API secret for your app", metavar="api-secret", required=False, dest=api_secret_name)
parser.add_argument("--%s"%("path"), help="the path to you unzipped parse export", metavar="path", required=False, dest=path_name)
parser.add_argument("--verbose", help="output details on requests made to stackmob", dest="verbose", type=bool)


def signal_handler(signal, frame):
	print ""
	sys.exit(0)

def print_resp(resp):
	print textwrap.dedent("""
	response code: %d
	response headers: %s
	response body: %s
	"""%(resp.status_code, resp.headers, resp.text))


def iso_date_to_unix(date):
	return int(dateutil.parser.parse(date).strftime("%s") + "000")

def rename_and_reformat_date(obj, old, new):
	date = obj.pop(old, None)
	object[new] = iso_date_to_unix(date)

def get_objects_from_file(path, file):
	f = open("%s/%s"%(path, file), "r")
	parse_json = json.loads(f.read())
	f.close()
	return parse_json["results"]

def get_schema_name(filename):
	# cut off .json
	name = filename[:-5] 
	# convert to the default user schema
	if name == "_User":
		return "user"
	else:
		return name

def sort_out_relations(theSchema, schemas, joins):
	relations = {}
	typeHints = {}

	schema_joins = filter(lambda x: x.endswith(theSchema), joins)
	# collate the join tables to be appended to the appropriate objects
	# later on. In StackMob relations are fields on the object
	for join in schema_joins:
		field = join.split(":")[1]
		relation_objects = get_objects_from_file(path, join)

		# group the relations by owner. This would be a one
		# liner if I knew the functional way to do this in python
		relation_map = {}
		for relation in relation_objects:
			owner_id = relation["owningId"]
			related_id = relation["relatedId"]
			if owner_id in relation_map:
				relation_map[owner_id].append(related_id)
			else:
				relation_map[owner_id] = [related_id]
		relations[field] = relation_map


		# This bit is unfortunate. The export data give no indication
		# what schema each relation goes to. This works for Parse
		# because their ids are unique per-app, but ours are per-schema
		# the only way to get the schema is to match up a related id with
		# an id from one of the schemas
		if len(relation_map) > 0:
			arbitrary_id = relation_map[relation_map.keys()[0]][0]
			for schema in schemas:
				objects = get_objects_from_file(path, schema)
				for object in objects:
					if object["objectId"] == arbitrary_id:
						typeHints[field] = get_schema_name(schema)
						break

	return relations, typeHints



if __name__=="__main__":
	signal.signal(signal.SIGINT, signal_handler)
	args = parser.parse_args()
	api_key, api_secret, path = read_api_key_and_secret_or_die(vars(args))

	debug_level = 0
	if(args.verbose):	
		print "using verbose mode"
		debug_level = 1
	
	api_client = client.DatastoreClient(api_key, api_secret, debug_level=debug_level)

	exported_files = os.listdir(path)

	# Relations, stored separately from schemas
	joins = filter(lambda x: x.startswith("_Join"), exported_files)

	# Regular schemas
	schemas = filter(lambda x: x == "_User.json" or not x.startswith("_"), exported_files)

	for schema in schemas:
		schema_name = get_schema_name(schema)
		schema_id = schema_name + "_id"

		relations, typeHints = sort_out_relations(schema, schemas, joins)

		objects = get_objects_from_file(path, schema)
		for object in objects:
			# rename the id
			object[schema_id] = object.pop("objectId", None)

			# reformat and rename the timestamps
			rename_and_reformat_date(object, "createdAt", "createddate")
			rename_and_reformat_date(object, "updatedAt", "lastmoddate")


			# convert over complex datatypes
			for field in object.keys():
				value = object[field]
				if isinstance(value, dict):
					subobject = value
					if "__type" in subobject:
						type = subobject["__type"]
						if type == "File":
							object.pop(field, None)
							print "Skipping field %s. Files are not yet supported"%(field)
						elif type == "GeoPoint":
							object.pop(field, None)
							lat = subobject["latitude"]
							lon = subobject["longitude"]
							# just differe lat/lon names
							object[field] = {"lat": lat, "lon": lon}
						elif type == "Date":
							object.pop(field, None)
							date_string = subobject["iso"]
							object[field] = iso_date_to_unix(date_string) 
						elif type == "Pointer":
							object.pop(field, None)
							print "Skipping field %s. Pointers are not yet supported"%(field)
						else:
							object.pop(field, None)
							print "Skipping field %s. Subobjects are not yet supported"%(field)

			# save the data twice so the timestamp fields get set. This is
			# a bug that probably only would be noticed in a migration
			api_client.post(schema_name, object, "")
			print api_client.post(schema_name, object, "")

#for join in joins:
		# we can get the name of the field and the schema from the join filename
#parts = join.split(".")[0].split(":")
#field = parts[1]
#owner_schema = parts[2]

		# however, there's no indication what schema it goes to. Parse seems to have
		# globally unique ids, we do not. Figuring out the related schema is a bit hacky






